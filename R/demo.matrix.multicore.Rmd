---
title: "Multicore matrix computation demo"
author: "Peter Carbonetto"
date: September 25, 2017
output:
  html_document:
    theme: readable
    include:
      before_body: include/header.html
---

Give overview of the analysis here. Does not work in Windows.

```{r set-chunk-options, include=FALSE}
knitr::opts_chunk$set(
  comment = "#",
  collapse = TRUE,
  fig.align = "center",
  tidy = FALSE,
  fig.path = paste0("figure/",knitr::current_input(),"/"))
```

## Initial setup

1. Install the [cfwlab package](https://github.com/pcarbo/cfwlab).

2. Download source file
   [setblas.c](https://github.com/pcarbo/pcarbo/blob/master/R/setblas.c)
   and build the shared object file in the same directory as your
   R working directory: `R CMD SHLIB setblas.c`

3. If you logged in to a compute cluster, request a compute node and
   request resources for your computing. For example, on the RCC
   compute cluster, I requested a compute node with 8 GB of memory and
   10 cores (CPUs):
   `sinteractive --partition=broadwl --cpus-per-task=10 --mem=20G`

4. Set the number of threads used by the BLAS library installed with
   R. For example, if you have installed R with OpenBLAS, you can set
   the number of threads to 2 with this command in the bash shell:
   `export OPENBLAS_NUM_THREADS=2`.

5. Start up R or RStudio.

## Setting up your R environment

Load the parallel package, cfwlab package, and the "setblas" function.

```{r load-pkgs}
library(parallel)
library(cfwlab)
dyn.load("setblas.so")
```

Initialize the random number generator.

```{r set-seed}
set.seed(1)
```

## Function definitions

Here we define a few functions that will be useful below in our
experiments. You don't have to understand the code---just make sure
that these functions are defined in your environment to run the
examples below.

This function distributes the elements of `x` evenly (or as evenly as
possible) into `k` list elements.

```{r distribute}
distribute <- function (x, k)
  split(x,rep(1:k,length.out = length(x)))
```

This function replicates vector `x` to create an `n x m` matrix,
where `m = length(x)`.

```{r reprow}
rep.row <- function (x, n)
  matrix(x,n,length(x),byrow = TRUE)
```

This function sets the number of threads used by OpenBLAS.

```{r}
set.blas.num.threads <- function (n) {
  .Call("set_blas_Call",n = as.integer(n))
  return(n)
}
```

This function takes as input an array of unnormalized log-importance
weights and returns normalized importance weights such that the sum of
the normalized importance weights is equal to 1. We guard against
underflow or overflow by adjusting the log-importance weights so that
the largest importance weight is 1.

```{r normalizelogweights}
normalizelogweights <- function (logw) {
  c <- max(logw)
  w <- exp(logw - c)
  return(w / sum(w))
}
```

This function computes the marginal log-likelihood the regression
model of Y given X assuming that the prior variance of the regression
coefficients is sa. Here K is the "kinship" matrix, `K =
tcrossprod(X)/p`. Also, H is the covariance matrix of Y divided by
residual variance.

```{r computelogweight}
compute.log.weight <- function (K, y, sa, use.backsolve = TRUE) {
  H <- diag(n) + sa*K
  L <- t(tryCatch(chol(H),error = function(e) FALSE))
  if (is.matrix(R)) {
    if (use.backsolve)
      x <- backsolve(t(L),forwardsolve(L,y))
    else
      x <- solve(H,y)
    logw <- (-determinant(sum(y*x)*H,logarithm = TRUE)$modulus/2)
  } else
    logw <- 0
  return(logw)
}
```

This function computes the marginal log-likelihood for multiple
settings of the prior variance parameter.

```{r computelogweights}
compute.log.weights <- function (K, y, sa, use.backsolve = TRUE)
  sapply(as.list(sa),function (x) compute.log.weight(K,y,x,use.backsolve))
```

This is a multicore variant of the above function implemented using
the "mclapply" function. Input argument `nc` specifies the number of
cores (CPUs) to use for the computation. Note that the mclapply relies
on forking and therefore will not work on a computer running Windows.

```{r computelogweights-mulicore}
compute.log.weights.multicore <- function (K, y, sa, nc = 2,
                                           use.backsolve = TRUE) {
  # n     <- length(sa)
  samples <- distribute(1:length(sa),nc)
  logw    <- mclapply(samples,
               function (i) compute.log.weights(K,y,sa[i],use.backsolve),
               mc.cores = nc)
  logw <- do.call(c,logw)
  logw[unlist(samples)] <- logw
  return(logw)
}
```

## Load data

Load the phenotype and genotype data. Here, we analyze soleus muscle
weight, but other phenotypes are available in this data set.

```{r load-data}
data(cfw.pheno)
data(cfw.geno)
X <- cfw.geno
y <- cfw.pheno[["soleus"]]
rm(cfw.geno,cfw.pheno)
```

Remove the rows containing missing data.

```{r filter-missing}
rows <- which(!is.na(y))
y    <- y[rows]
X    <- X[rows,]
```

## Preprocess data

Center y and the columns of X.

```{r center-data}
n <- nrow(X)
p <- ncol(X)
X <- X - rep.row(colMeans(X),n)
y <- y - mean(y)
```

Compute the kinship matrix. This computation may take a minute or two.

```{r compute-kinship}
K <- tcrossprod(X)/p
```

## Estimate PVE

In this section, we will compute an importance sampling estimate of
the proportion of variance in the quantitative trait explained by the
available genotypes (abbreviated as "PVE"). This is a numerically
intensive operation because it involves factorizing a large, symmetric
matrix separately for each Monte Carlo sample. We will explore how
increasing the number of threads available for the matrix computations
(BLAS) and for the Monte Carlo computations improves the computation
time.

First, draw samples of the PVE estimate from the proposal
distribution, which is uniform on [0,1]. Here, we compute the
importance sampling estimate using 1,000 samples, but you can choose a
larger or smaller number of samples.

```{r draw-samples}
ns <- 1000
h  <- runif(ns)
```

For each PVE setting, get the prior variance of the regression
coefficients assuming a fully "polygenic" model.

```{r get-prior-settings}
sx <- sum(apply(X,2,sd)^2)
sa <- p*h/(1-h)/sx
```

Now we reach the numerically intensive part. First, let's compute the
importance weights without taking advantage of any multithreaded or
parallel computing capabilities. Since I am using OpenBLAS, I can set
the number of matrix operation threads using `set.blas.num.threads`,
but you may need to set it a different way.

```{r compute-importance-weights-1}
# set.blas.num.threads(1)
# system.time(logw <- compute.log.weights(K,y,sa,use.backsolve = TRUE))
```

The "elapsed" time tells us the amount of time it took to compute the
importance weights.

Also, here I am using a "trick" to improve the speed of one of the
matrix operations, and you can set `use.backsolve = FALSE` to see how
much of an improvement this trick gives you.

On my compute node, I have 10 CPUs available, so let's speed up the
matrix computations by using all 10 CPUs:

```{r compute-importance-weights-2}
# set.blas.num.threads(10)
# system.time(logw <- compute.log.weights(K,y,sa))
```

We've obtained a substantial improvement in computation time, but it
is far from the "ideal" of 10x improvement. That is because there is a
significant amount of overhead in parallelizing these computations.

**Exercise:** Experiment with different numbers of threads to compare
performance gains.

Next, let's try a different parallelization scheme, in which we
compute batches of importance weights in parallel. The last input
argument specifies the number of threads; in my case, 10 seems ideal
because I have 10 CPUs available to use:

```{r compute-importance-weights-3}
# set.blas.num.threads(1)
# system.time(logw <- compute.log.weights.multicore(K,y,sa,nc = 10))
```

Computing the importance weights in parallel---with no multithreading
for the matrix (BLAS) operations---seems to be a more efficient use of
the computing resources.

We can of course combine the two types of multithreading, e.g.:

```{r compute-importance-weights-4}
# set.blas.num.threads(2)
# system.time(logw <- compute.log.weights.multicore(K,y,sa,nc = 5))
```

Once we have done the hard work of computing the importance weights,
we can quickly compute a Monte Carlo estimate of the posterior mean
PVE:

```{r compute-posterior-stats}
# w <- normalizelogweights(logw)
# sum(w * h)
```

## Session information

This is the operating system, version of R and the packages that were
used to generate these results given above.

```{r session-info}
sessionInfo()
```
