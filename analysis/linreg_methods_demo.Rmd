---
title: "Exploring methods for large-scale multiple linear regression"
author: Peter Carbonetto
output: workflowr::wflow_html
---

*Add overview here.*

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

Load the packages and set the seed so that we may reproduce the
results more easily.

```{r load-pkgs, message=FALSE}
library(MASS)
library(L0Learn)
library(bayeslm)
library(dlbayes)
library(SSLASSO)
library(EMVS)
library(R.matlab)
library(ggplot2)
library(cowplot)
set.seed(1)
source("../R/linreg_methods_demo_functions.R")
```

Simulate a data set with correlated variables in a similar way to
Example 1 from [Zou & Hastie 2005][zh2005].

```{r sim-data}
simulate_predictors_decaying_corr <- function (n, p, s = 0.5)
  return(mvrnorm(n,rep(0,p),s^abs(outer(1:p,1:p,"-"))))
simulate_outcomes <- function (X, b, se)
  return(drop(X %*% b - 1 + rnorm(nrow(X),sd = se)))
p      <- 1000
se     <- 3
b      <- rep(0,p)
b[1:3] <- c(3,1.5,2)
Xtrain <- simulate_predictors_decaying_corr(200,p,0.5)
Xtest  <- simulate_predictors_decaying_corr(500,p,0.5)
train  <- list(X = Xtrain,y = simulate_outcomes(Xtrain,b,se))
test   <- list(X = Xtest,y = simulate_outcomes(Xtest,b,se))
btrue  <- b
```

Save the data to a MAT file for running the Trimmed Lasso. (The data
are centered because the Trimmed Lasso does not include an intercept.)

```{r save-mat-data}
writeMat("train.mat",
         X = scale(train$X,center = TRUE,scale = FALSE),
		 y = with(train,y - mean(y)))
```

L0Learn
-------

First, let's try the [L0Learn][l0learn] from
[Hazimeh and Mazumder 2020][hm2020]. In the
[vignette][l0learn-vignette], the authors suggest using the "L0L1" or
"L0L2" penalties for better predictive performance. The package
includes an interface to automatically select the penalty parameters
$\lambda, \gamma$ that minimize the test-set error. (L0Learn has two
model fitting algorithms; the CD algorithm is faster whereas the
CDPSI can sometimes produces better fits. For expediency we'll use
the CD algorithm.)

```{r l0learn}
l0learn_cv <- with(train,L0Learn.cvfit(X,y,penalty = "L0L1",algorithm = "CD"))
i      <- which.min(sapply(l0learn_cv$cvMeans,min))
j      <- which.min(l0learn_cv$cvMeans[[i]])
gamma  <- l0learn_cv$fit$gamma[i]
lambda <- l0learn_cv$fit$lambda[[i]][j]
```

Compare the coefficient estimates against the ground truth:

```{r plot-coef-l0learn, fig.height=2.5, fig.width=2.5}
b <- as.vector(coef(l0learn_cv,gamma = gamma,lambda = lambda))
b <- b[-1]
plot_coefs(btrue,b)
```

As expected, the L0 penalty shrinks most of the coefficients to zero.

The mean squared error (MSE) summarizes the accuracy of the
predictions in the test set examples:

```{r predict-l0learn, fig.height=2.5, fig.width=2.5}
y <- as.vector(predict(l0learn_cv,newx = test$X,gamma = gamma,lambda = lambda))
plot_responses(test$y,y)
```

Let's compare this to L0Learn with the simpler L0 penalty (which is a
special case of the L0L1 penalty in which $\gamma = 0$):

```{r l0learn-l0only, fig.height=2.5, fig.width=2.5}
l0learn_cv <- with(train,L0Learn.cvfit(X,y,penalty = "L0",algorithm = "CD"))
i <- which.min(l0learn_cv$cvMeans[[1]])
lambda <- l0learn_cv$fit$lambda[[1]][i]
y <- as.vector(predict(l0learn_cv,newx = test$X,lambda = lambda))
qplot(test$y,y) +
  geom_abline(intercept = 0,slope = 1,color = "deepskyblue",
              linetype = "dashed") +
  ggtitle(sprintf("mse = %0.3f",mse(test$y,y))) +
  theme_cowplot(font_size = 12) +
  theme(plot.title = element_text(face = "plain",size = 12))
```

Indeed, in this one example at least, L0Learn with the L0L1 penalty
has better prediction performance than the L0 penalty.

Trimmed Lasso
-------------

The Trimmed Lasso was described by
[Amir, Basri and Nadler 2021][abn2021]. It is implemented in
[MATLAB][trimmed-lasso], so there is a separate script,
`trimmed_lasso.m`, to run the method. Having run this script, we now
load the results:

```{r trimmed-lasso}
k <- c(1,3,10,20,100)
B <- readMat("trimmed_lasso_coefs.mat")$B
colnames(B) <- paste0("k",k)
b <-  B[,"k3"]
```

The Trimmed Lasso was run with different settings of the sparsity
parameter $k$. Here we take the setting of $k$ that is closest to
the true number of nonzero coefficients (which in this example is also
3). As expected, the coefficient estimates are very sparse:

```{r plot-coef-trimmed-lasso, fig.height=2.5, fig.width=2.5}
plot_coefs(btrue,b)
```

For prediction, we need to estimate the intercept. Here we compute the
MLE:

```{r trimmed-lasso-intercept}
b0 <- with(train,mean(y - X %*% b))
```

The Trimmed Lasso is well suited to this example because the true
coefficients are very sparse, and indeed the prediction accuracy is
very good:

```{r predict-trimmed-lasso, fig.height=2.5, fig.width=2.5}
y <- drop(b0 + test$X %*% b)
plot_responses(test$y,y)
```

One drawback with the Trimmed Lasso is that cross-validation will be
needed to get the right $k$. Since cross-validation is not implemented
in the software, we will have to do it ourselves.

The Horseshoe
-------------

Another option is multiple linear regression with the
[horseshoe prior][horseshoe].  There are several implementations in R
and MATLAB listed in [this review paper][horseshoe-review]. The
recent [bayeslm package][bayeslm] package implements an efficient
slice sampler for multiple linear regression with the horseshoe prior
and several other priors, so we'll try that.

```{r horseshoe-1, cache=TRUE}
horseshoe <- bayeslm(train$y,train$X,prior = "horseshoe",icept = TRUE,
                     verb = TRUE, standardize = FALSE,singular = TRUE,
                     burnin = 1000,N = 4000)
```

The Horseshoe also effectively shrank the coefficients and produced
accurate predictions in the example data set:

```{r horseshoe-2, fig.height=2.5, fig.width=5, results="hide"}
b <- colMeans(horseshoe$beta)
b <- b[-1]
y <- predict(horseshoe,X = test$X,burnin = 1000)
plot_grid(plot_coefs(btrue,b),
          plot_responses(test$y,y))
```

Dirichlet-Laplace
-----------------

Next I looked at the multiple linear regression with the
[Dirichlet-Laplace prior][dl]. It is implemented in the
[dlbayes package][dlbayes]. However, the package has a
[bug][dlbayes-bug], so you should use
[my fork of the dlbayes package][dlbayes-fork] which contains the bug
fix. To install this version of the package, run:

```{r install-dlbayes, eval=FALSE}
remotes::install_github("pcarbo/dlbayes",upgrade = "never")
```

Since the model does not include an intercept, we center the data
before performing the multiple linear regression analysis:

```{r dirichlet-laplace-1, cache=TRUE}
X_centered <- scale(train$X,center = TRUE,scale = FALSE)
y_centered <- with(train,y - mean(y))
dl_hyper <- dlhyper(X_centered,y_centered)
dl_out <- dl(X_centered,y_centered,burn = 1000,nmc = 4000,thin = 1,
             hyper = dl_hyper)
```
			 
Let's compare the coefficient estimates and predictions to the ground
truth:

```{r dirichlet-laplace-2, fig.height=2.5, fig.width=5}
b0 <- with(train,mean(y - X %*% b))
b <- dlanalysis(dl_out)$betamean
y <- drop(b0 + test$X %*% b)
plot_grid(plot_coefs(btrue,b),
          plot_responses(test$y,y))
```

The results are not impressive. However, it is possible that there are
better choices for the hyperparameter setting than the one given by
`dlhyper`.

SSLASSO
-------

Next let's try both the "adaptive" and "separable" variants of the
[Spike-and-Slab LASSO][sslasso]:

```{r sslasso-1}
sslasso_adapt <-
  with(train,SSLASSO(X,y,penalty = "adaptive",variance = "unknown"))
sslasso_sep <-
  with(train,SSLASSO(X,y,penalty = "separable",variance = "unknown"))
```

Let's take a look at the SSLASSO with separable penalty first. As far
as I can tell, the SSLASSO does not provide an automated way to select
the spike and slab penalties, so I'll choose one by hand:

```{r plot-coef-sslasso-sep, fig.height=2.5, fig.width=2.5}
b <- sslasso_sep$beta[,16]
qplot(btrue,b) +
  geom_abline(intercept = 0,slope = 1,color = "deepskyblue",
              linetype = "dashed") +
  theme_cowplot(font_size = 12)	 
```

Compare these estimates to the estimates obtained by the adaptive
SSLASSO:

```{r plot-coef-sslasso-adapt, fig.height=2.5, fig.width=2.5}
b <- sslasso_adapt$beta[,16]
qplot(btrue,b) +
  geom_abline(intercept = 0,slope = 1,color = "deepskyblue",
              linetype = "dashed") +
  theme_cowplot(font_size = 12)	 
```

Another annoying thing about the SSLASSO package is that it does not
provide a "predict" method. So we will compute the predictions by
hand after extracting the :

```{r predict-sslasso, fig.height=2.5, fig.width=5}
b0 <- sslasso_sep$intercept[16]
b  <- sslasso_sep$beta[,16]
y  <- drop(b0 + test$X %*% b)
p1 <- qplot(test$y,y) +
  geom_abline(intercept = 0,slope = 1,color = "deepskyblue",
              linetype = "dashed") +
  ggtitle(sprintf("separable (mse = %0.3f)",mse(test$y,y))) +
  theme_cowplot(font_size = 12)
b0 <- sslasso_adapt$intercept[16]
b  <- sslasso_adapt$beta[,16]
y  <- drop(b0 + test$X %*% b)
p2 <- qplot(test$y,y) +
  geom_abline(intercept = 0,slope = 1,color = "deepskyblue",
              linetype = "dashed") +
  ggtitle(sprintf("adaptive (mse = %0.3f)",mse(test$y,y))) +
  theme_cowplot(font_size = 12)
plot_grid(p1,p2)
```

EMVS
----

The [EMVS package][emvs] seems to be better documented (unfortunately,
it was removed from CRAN). It also has two variants with different
priors, the "independent" prior (which is recommended by the authors)
and the "conjugate" prior. Let's compare their performance in this
simulated example.

```{r emvs, results="hide"}
emvs_conj = with(train,
  EMVS(y,X,v0 = seq(0.1,2,length.out = 20),v1 = 10,independent = FALSE))
emvs_ind = with(train,
  EMVS(y,X,v0 = exp(seq(-18,-1,length.out = 20)),v1 = 1,independent = TRUE))
```

*Add text here.*

```{r emvs-conj, fig.height=2.5, fig.width=2.5}
i  <- which.max(emvs_conj$log_g_function)
b  <- with(emvs_conj,betas[i,] * prob_inclusion[i,])
qplot(btrue,b) +
  geom_abline(intercept = 0,slope = 1,color = "deepskyblue",
              linetype = "dashed") +
  theme_cowplot(font_size = 12)
```

*Add text here.*

```{r emvs-ind, fig.height=2.5, fig.width=2.5}
b <- with(emvs_ind,betas[14,] * prob_inclusion[14,])
qplot(btrue,b) +
  geom_abline(intercept = 0,slope = 1,color = "deepskyblue",
              linetype = "dashed") +
  theme_cowplot(font_size = 12)
```

*Add text here.*

```{r predict-emvs, fig.height=2.5, fig.width=5}
b0 <- emvs_conj$intersects[i]
b  <- emvs_conj$betas[i,]
y  <- drop(b0 + test$X %*% b)
p1 <- qplot(test$y,y) +
  geom_abline(intercept = 0,slope = 1,color = "deepskyblue",
              linetype = "dashed") +
  ggtitle(sprintf("conjugate (mse = %0.3f)",mse(test$y,y))) +
  theme_cowplot(font_size = 12)
b0 <- emvs_ind$intersects[14]
b  <- emvs_ind$beta[14,]
y  <- drop(b0 + test$X %*% b)
p2 <- qplot(test$y,y) +
  geom_abline(intercept = 0,slope = 1,color = "deepskyblue",
              linetype = "dashed") +
  ggtitle(sprintf("independent (mse = %0.3f)",mse(test$y,y))) +
  theme_cowplot(font_size = 12)
plot_grid(p1,p2)
```

[emvs]: https://cran.r-project.org/package=EMVS
[sslasso]: https://cran.r-project.org/package=SSLASSO
[zh2005]: https://doi.org/10.1111/j.1467-9868.2005.00503.x
[hm2020]: https://doi.org/10.1287/opre.2019.1919
[abn2021]: https://doi.org/10.1137/20M1330634
[l0learn]: https://cran.r-project.org/package=L0Learn
[l0learn-vignette]: https://cran.r-project.org/web/packages/L0Learn/vignettes/L0Learn-vignette.html
[trimmed-lasso]: https://github.com/tal-amir/sparse-approximation-gsm
[horseshoe]: https://doi.org/10.1093/biomet/asq017
[horseshoe-review]: https://doi.org/10.1214/19-STS700
[bayeslm]: https://doi.org/10.1080/10618600.2018.1482762
[dl]: https://doi.org/10.1080/01621459.2014.960967
[dlbayes]: https://github.com/xylimeng/BayesianVariableSelection
[dlbayes-bug]: https://github.com/xylimeng/BayesianVariableSelection/issues/1
[dlbayes-fork]: https://github.com/pcarbo/dlbayes
