---
title: "Exploring methods for large-scale multiple linear regression"
author: Peter Carbonetto
output: workflowr::wflow_html
---

The goal here is to try out several recent R packages implementing
methods for large-scale multiple linear regression, including L0Learn,
the Spike-and-Slab LASSO, the Trimmed Lasso, EMVS, and methods with the
Horseshoe and Dirichlet-Laplace priors.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

Load the packages and set the seed so that we may reproduce the
results more easily.

```{r load-pkgs, message=FALSE}
library(MASS)
library(L0Learn)
library(horseshoe)
library(bayeslm)
library(dlbayes)
library(SSLASSO)
library(EMVS)
library(R.matlab)
library(ggplot2)
library(cowplot)
set.seed(1)
source("../R/linreg_methods_demo_functions.R")
```

Simulate a data set with correlated variables in a similar way to
Example 1 from [Zou & Hastie 2005][zh2005].

```{r sim-data}
simulate_predictors_decaying_corr <- function (n, p, s = 0.5)
  return(mvrnorm(n,rep(0,p),s^abs(outer(1:p,1:p,"-"))))
simulate_outcomes <- function (X, b, se)
  return(drop(X %*% b - 1 + rnorm(nrow(X),sd = se)))
p      <- 1000
se     <- 3
b      <- rep(0,p)
b[1:3] <- c(3,1.5,2)
Xtrain <- simulate_predictors_decaying_corr(200,p,0.5)
Xtest  <- simulate_predictors_decaying_corr(500,p,0.5)
train  <- list(X = Xtrain,y = simulate_outcomes(Xtrain,b,se))
test   <- list(X = Xtest,y = simulate_outcomes(Xtest,b,se))
btrue  <- b
```

Save the data to a MAT file for running the Trimmed Lasso. (The data
are centered because the Trimmed Lasso does not include an intercept.)

```{r save-mat-data}
writeMat("train.mat",
         X = scale(train$X,center = TRUE,scale = FALSE),
		 y = with(train,y - mean(y)))
```

L0Learn
-------

First, let's try the [L0Learn][l0learn] from
[Hazimeh & Mazumder 2020][hm2020]. In the
[vignette][l0learn-vignette], the authors suggest using the "L0L1" or
"L0L2" penalties for better predictive performance. The package
includes an interface to automatically select the penalty parameters
$\lambda, \gamma$ that minimize the test-set error. (L0Learn has two
model fitting algorithms; the CD algorithm is faster whereas the CDPSI
can sometimes produces better fits. For expediency we'll use the CD
algorithm.)

```{r l0learn}
l0learn_cv <- with(train,L0Learn.cvfit(X,y,penalty = "L0L1",algorithm = "CD"))
i      <- which.min(sapply(l0learn_cv$cvMeans,min))
j      <- which.min(l0learn_cv$cvMeans[[i]])
gamma  <- l0learn_cv$fit$gamma[i]
lambda <- l0learn_cv$fit$lambda[[i]][j]
```

Compare the coefficient estimates against the ground truth:

```{r plot-coef-l0learn, fig.height=2.5, fig.width=2.5}
b <- as.vector(coef(l0learn_cv,gamma = gamma,lambda = lambda))
b <- b[-1]
plot_coefs(btrue,b)
```

As expected, the L0 penalty shrinks most of the coefficients to zero.

The mean squared error (MSE) summarizes the accuracy of the
predictions in the test set examples:

```{r predict-l0learn, fig.height=2.5, fig.width=2.5}
y <- as.vector(predict(l0learn_cv,newx = test$X,gamma = gamma,lambda = lambda))
plot_responses(test$y,y)
```

Let's compare this to L0Learn with the simpler L0 penalty (which is a
special case of the L0L1 penalty in which $\gamma = 0$):

```{r l0learn-l0only, fig.height=2.5, fig.width=2.5}
l0learn_cv <- with(train,L0Learn.cvfit(X,y,penalty = "L0",algorithm = "CD"))
i <- which.min(l0learn_cv$cvMeans[[1]])
lambda <- l0learn_cv$fit$lambda[[1]][i]
y <- as.vector(predict(l0learn_cv,newx = test$X,lambda = lambda))
qplot(test$y,y) +
  geom_abline(intercept = 0,slope = 1,color = "deepskyblue",
              linetype = "dashed") +
  ggtitle(sprintf("mse = %0.3f",mse(test$y,y))) +
  theme_cowplot(font_size = 12) +
  theme(plot.title = element_text(face = "plain",size = 12))
```

Indeed, in this one example at least, L0Learn with the L0L1 penalty
has better prediction performance than the L0 penalty.

Trimmed Lasso
-------------

The Trimmed Lasso was described by
[Amir, Basri & Nadler 2021][abn2021]. It is implemented in
[MATLAB][trimmed-lasso], so there is a separate script,
`trimmed_lasso.m`, to run the method. Having run this script, we now
load the results:

```{r trimmed-lasso}
k <- c(1,3,10,20,100)
B <- readMat("trimmed_lasso_coefs.mat")$B
colnames(B) <- paste0("k",k)
b <-  B[,"k3"]
```

The Trimmed Lasso was run with different settings of the sparsity
parameter $k$. Here we take the setting of $k$ that is closest to
the true number of nonzero coefficients (which in this example is also
3). As expected, the coefficient estimates are very sparse:

```{r plot-coef-trimmed-lasso, fig.height=2.5, fig.width=2.5}
plot_coefs(btrue,b)
```

For prediction, we need to estimate the intercept. Here we compute the
MLE:

```{r trimmed-lasso-intercept}
b0 <- with(train,mean(y - X %*% b))
```

The Trimmed Lasso is well suited to this example because the true
coefficients are very sparse, and indeed the prediction accuracy is
very good:

```{r predict-trimmed-lasso, fig.height=2.5, fig.width=2.5}
y <- drop(b0 + test$X %*% b)
plot_responses(test$y,y)
```

One drawback with the Trimmed Lasso is that cross-validation will be
needed to get the right $k$. Since cross-validation is not implemented
in the software, we will have to do it ourselves.

The Horseshoe
-------------

Another option is multiple linear regression with the
[horseshoe prior][horseshoe]. There are several implementations in R
and MATLAB listed in [this review paper][horseshoe-review]. The
recent [bayeslm package][bayeslm] package implements an efficient
slice sampler for multiple linear regression with the horseshoe prior
and several other priors, so we'll try that first.

```{r horseshoe-1, cache=TRUE}
horseshoe <- bayeslm(train$y,train$X,prior = "horseshoe",icept = TRUE,
                     verb = TRUE, standardize = FALSE,singular = TRUE,
                     burnin = 1000,N = 4000)
```

bayeslm also effectively shrank the coefficients and produced accurate
predictions in the example data set:

```{r horseshoe-2, fig.height=2.5, fig.width=5, results="hide"}
b <- colMeans(horseshoe$beta)
b <- b[-1]
y <- predict(horseshoe,X = test$X,burnin = 1000)
plot_grid(plot_coefs(btrue,b),
          plot_responses(test$y,y))
```

Next, let's try the [horseshoe package][horseshoe-package]:

```{r horseshoe-3, cache=TRUE}
X_centered <- scale(train$X,center = TRUE,scale = FALSE)
y_centered <- with(train,y - mean(y))
hs <- horseshoe(y_centered,X_centered,method.tau = "halfCauchy",
                method.sigma = "Jeffreys",burn = 1000,nmc = 4000)
```

It seems to work reasonably well although in this example the bayeslm
package gave the slightly better result (perhaps because the MCMC
algorithm in bayeslm is more efficient):

```{r horseshoe-4, fig.height=2.5, fig.width=5}
b0 <- with(train,mean(y - X %*% b))
b  <- hs$BetaHat
y  <- drop(b0 + test$X %*% b)
plot_grid(plot_coefs(btrue,b),
          plot_responses(test$y,y))
```

Dirichlet-Laplace
-----------------

Next I looked at the multiple linear regression with the
[Dirichlet-Laplace prior][dl]. It is implemented in the
[dlbayes package][dlbayes]. However, the package has a
[bug][dlbayes-bug], so you should use
[my fork of the dlbayes package][dlbayes-fork] which contains the bug
fix. To install this version of the package, run:

```{r install-dlbayes, eval=FALSE}
remotes::install_github("pcarbo/dlbayes",upgrade = "never")
```

Since the model does not include an intercept, we center the data
before performing the multiple linear regression analysis:

```{r dirichlet-laplace-1, cache=TRUE, eval=FALSE}
dl_hyper <- dlhyper(X_centered,y_centered)
dl_out <- dl(X_centered,y_centered,burn = 1000,nmc = 4000,thin = 1,
             hyper = dl_hyper)
```
			 
Let's compare the coefficient estimates and predictions to the ground
truth:

```{r dirichlet-laplace-2, fig.height=2.5, fig.width=5, eval=FALSE}
b0 <- with(train,mean(y - X %*% b))
b  <- dlanalysis(dl_out)$betamean
y  <- drop(b0 + test$X %*% b)
plot_grid(plot_coefs(btrue,b),
          plot_responses(test$y,y))
```

The results are not impressive. However, it is possible that there are
better choices for the hyperparameter setting than the one given by
`dlhyper`.

SSLASSO
-------

Now let's try the [Spike-and-Slab LASSO][sslasso], both the "adaptive"
and "separable" variants. For the SSLASSO with the separable penalty,
it would be a bit of an unfair advantage to set the prior inclusion
probability $\theta$ to the true value, so here we set it to a value
(0.05) larger than the true value (0.003).

```{r sslasso-1}
sslasso_sep <-
with(train,SSLASSO(X,y,penalty = "separable",variance = "unknown",
                   theta = 0.05))
sslasso_adapt <-
  with(train,SSLASSO(X,y,penalty = "adaptive",variance = "unknown",
                     lambda0 = seq(3,200,length.out = 100),lambda1 = 3))
```

Note that there is a bug in SSLASSO: To override the default choice
for `lambda1`, you need to specify both `lambda0` and `lambda1`.

Let's take a look at the SSLASSO with separable penalty first. As far
as I can tell, the SSLASSO does not provide an automated way to select
the spike penalty parameter $\lambda_0$, so here, to keep things
simple, I choose this penalty by hand. (In general we would probably
want to implement a simple cross-validation scheme to choose this
penalty, as well as the slab penalty $\lambda_1$, although
[Rockova & George 2018][rg2018] observe that the performance is not
very sensitive to the choice of $\lambda_1$, so it seems reasonable to
try a small number of settings, e.g., $\lambda_1 = \{0.1, 0.5, 1, 5,
10\}$, similar to what was done in the SSLASSO paper.)

```{r plot-coef-sslasso-sep, fig.height=2.5, fig.width=2.5}
i <- 8
b <- sslasso_sep$beta[,i]
plot_coefs(btrue,b)
```

Compare these estimates to the estimates obtained by the adaptive
SSLASSO:

```{r plot-coef-sslasso-adapt, fig.height=2.5, fig.width=2.5}
b <- sslasso_adapt$beta[,i]
plot_coefs(btrue,b)
```

Observe that the adaptive variant did a much better job shrinking the
small coefficients to zero. Both methods accurately estimated the
residual variance:

```{r sslasso-sigma}
tail(sslasso_sep$sigmas,n = 1)
tail(sslasso_adapt$sigmas,n = 1)
```

SSLASSO package does not provide a "predict" method so we need to
compute the predictions by hand after extracting the coefficient
estimates.

```{r predict-sslasso, fig.height=2.5, fig.width=5}
b0 <- sslasso_sep$intercept[i]
b  <- sslasso_sep$beta[,i]
y  <- drop(b0 + test$X %*% b)
p1 <- plot_responses(test$y,y)
b0 <- sslasso_adapt$intercept[i]
b  <- sslasso_adapt$beta[,i]
y  <- drop(b0 + test$X %*% b)
p2 <- plot_responses(test$y,y)
plot_grid(p1,p2)
```

In this example, the adaptive penalty performed a bit better than the
separable penalty with $\theta = 0.05$.

EMVS
----

Finally, let's try the [EMVS method][emvs] implemented the
[EMVS package][emvs-package]. It seems to be fairly well documented
(it has a vignette at least). Unfortunately, it was removed from
CRAN. It also has two variants with different priors, the
"independent" prior (which is recommended by the authors) and the
"conjugate" prior. The "backward" option for simulated annealing is
recommended. Let's compare the performance of the two variants in the
simulated example.

```{r emvs-1, results="hide"}
v0 <- exp(seq(-10,-1,length.out = 20))
emvs_conj <- EMVS(train$y,train$X,v0 = v0,v1 = 1,independent = FALSE,
                  direction = "backward",standardize = FALSE,epsilon = 1e-6)
emvs_ind <- EMVS(train$y,train$X,v0 = v0,v1 = 1,independent = TRUE,
                 direction = "backward",standardize = FALSE,epsilon = 1e-6)
```

For both variants, a variety of settings for the "spike" parameter
$v_0$ can be chosen to explore the results at different settings,
similar to how the results of the Lasso are sometimes explored by
varying the penalty strength parameter $\lambda$.

The EMVS coefficients can be thresholded using the posterior inclusion
probabilities. A convenient feature of the "conjugate" prior is that
it gives a posterior probability for each setting of $v_0$ which can
be used to select the best setting:

```{r emvs-2, fig.height=2.5, fig.width=2.5}
i  <- which.max(emvs_conj$log_g_function)
b0 <- emvs_conj$intersects[i]
b  <- emvs_conj$betas[i,]
js <- which(emvs_conj$prob_inclusion[i,] < 0.5)
b[js] <- 0
emvs_conj$b0 <- b0
emvs_conj$b  <- b
plot_coefs(btrue,b)
```

The independent prior does not provide an automatic way to select
$v_0$, so instead here we choose it by hand (more generally, we may
want to implement some sort of cross-validation to choose $v_0$).

```{r emvs-ind, fig.height=2.5, fig.width=2.5}
i  <- 1
b0 <- emvs_ind$intersects[i]
b  <- emvs_ind$betas[i,]
js <- which(emvs_ind$prob_inclusion[i,] < 0.5)
b[js] <- 0
emvs_ind$b0 <- b0
emvs_ind$b  <- b
plot_coefs(btrue,b)
```

As the authors indicated, the independent prior produces much better
predictions than the conjugate prior. Still, even the better
performing variant of EMVS gives among the worse results for this data
set.

```{r predict-emvs, fig.height=2.5, fig.width=5}
y_conj <- drop(with(emvs_conj,b0 + test$X %*% b))
y_ind <- drop(with(emvs_ind,b0 + test$X %*% b))
p1 <- plot_responses(test$y,y_conj)
p2 <- plot_responses(test$y,y_ind)
plot_grid(p1,p2)
```

[emvs]: https://cran.r-project.org/package=EMVS
[sslasso]: https://cran.r-project.org/package=SSLASSO
[zh2005]: https://doi.org/10.1111/j.1467-9868.2005.00503.x
[hm2020]: https://doi.org/10.1287/opre.2019.1919
[rg2018]: https://doi.org/10.1080/01621459.2016.1260469
[abn2021]: https://doi.org/10.1137/20M1330634
[l0learn]: https://cran.r-project.org/package=L0Learn
[l0learn-vignette]: https://cran.r-project.org/web/packages/L0Learn/vignettes/L0Learn-vignette.html
[trimmed-lasso]: https://github.com/tal-amir/sparse-approximation-gsm
[horseshoe]: https://doi.org/10.1093/biomet/asq017
[horseshoe-review]: https://doi.org/10.1214/19-STS700
[horseshoe-package]: https://cran.r-project.org/package=horseshoe
[emvs]: https://doi.org/10.1080/01621459.2013.869223
[emvs-package]: https://cran.r-project.org/package=EMVS
[bayeslm]: https://doi.org/10.1080/10618600.2018.1482762
[dl]: https://doi.org/10.1080/01621459.2014.960967
[dlbayes]: https://github.com/xylimeng/BayesianVariableSelection
[dlbayes-bug]: https://github.com/xylimeng/BayesianVariableSelection/issues/1
[dlbayes-fork]: https://github.com/pcarbo/dlbayes
