---
title: Illustration of SuSiE in a "difficult" situation
author: Peter Carbonetto
output: workflowr::wflow_html
---

Here we explore the behaviour of SuSiE in a toy example where the
correlations between the predictors are not so straightforward. We
will see that SuSiE still achieves the goal of correctly inferring
Credible Sets in this slightly more complicated example.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold")
```

Load packages
-------------

The mvtnorm package is used to simulate the data.

```{r load-pkgs}
library(mvtnorm)
library(varbvs)
library(susieR)
```

Overview
--------

One of the referees was interested in understanding the behaviour of
SuSiE in the following scenario in which the correlation structure is
slightly more complex than the motivating example provided in the
paper.

Consider 5 correlated predictors in a linear regression, $x_1, \ldots,
x_5$, in which only two variables, $x1$ and $x3$, have an effect on
the outcome, $y$; that is, their regression coefficients $b_1$ and
$b_3$ are not zero. The first two predictors are very strongly
correlated with each other, and the third and fourth predictors are
also very strongly correlated. Therefore, it will be difficult to
distinguish between $x1$ and $x2$, and the same for $x3$ and
$x4$. This correlation structure is the same as the simple motivating
example given in the paper.

To complicate the example, there is a fifth predictor that has no
effect on the outcome, and is also strongly correlated with the other
variables. This is the correlation matrix for the 5 predictors:

```{r correlation-matrix}
S <- rbind(c(   1, 0.99,  0.8,  0.8, 0.9),
           c(0.99,    1,  0.8,  0.8, 0.9),
           c( 0.8,  0.8,    1, 0.99, 0.9),
           c( 0.8,  0.8, 0.99,    1, 0.9),
           c( 0.9,  0.9,  0.9,  0.9,   1))
```

Therefore, without an abundance of data, it will be difficult to
distinguish between $(x_1, x_2, x_3)$ and $(x_3, x_4, x_5)$. The
inferential statement that would best capture our uncertainty about
which variables affect the outcome would look something like this:

$$\mbox{Among coefficients $\beta_1, \ldots, \beta_5$, two (or more)
are not zero}$$

SuSiE is not designed to produce such inferential statements,
but it could state instead:

$$(\beta_1 \neq 0 \mbox{ or } \beta_2 \neq 0 \mbox{ or } \beta_5 \neq
0) \mbox{ and } (\beta_3 \neq 0 \mbox{ or } \beta_4 \neq 0)$$

Another equally valid statement could be:

$$(\beta_1 \neq 0 \mbox{ or } \beta_2 \neq 0) \mbox{ and } (\beta_3
\neq 0 \mbox{ or } \beta_4 \neq 0 \mbox{ or } \beta_5 \neq 0)$$

Although neither of these statements completely capture our
uncertainty about which variables affect the outcome, it does satisfy
the following aim, which was given in the introduction to the paper:

*A key feature of our method, which distinguishes it from most
existing BVSR methods, is that it produces "Credible Sets" of
variables that quantify uncertainty in which variable should be
selected when multiple, highly correlated variables compete with one
another. These Credible Sets are designed to be as small as possible
while still each capturing a relevant variable.

The key point here is that the Credible Sets are not necessarily
intended to capture the full uncertainty in which variables should be
selected, but does at least capture *all relevant variables*.

Below, we investigate these claims empirically.

Simulate data
-------------

We simulate 150 samples in this scenario.

```{r simulate-data}
set.seed(1)
n <- 200
b <- c(1, 0, -1, 0, 0)
X <- rmvnorm(n,sigma = S)
y <- drop(X %*% b + rnorm(n)/0.3)
```

Multiple regression using varbvs
--------------------------------

First, to provide a counterexample that does not accomplish what we
set out to achieve, we fit the "varbvs" model, and show that model,
although very similar to SuSiE in many ways, does not adequately
capture uncertainty in the selected variables.

```{r fit-varbvs}
fit1 <- varbvs(X,NULL,y,logodds = log10(0.5),sa = 1,verbose = FALSE)
print(summary(fit1)$top.vars)
```

From this summary, we see that varbvs correctly determines that two
variables are useful for predicting the outcome, but it incorrectly
places all the weight on the first and third variables. This is
expected behaviour due to the particular variational approximation
used in varbvs, but it is undesirable when we want to better quantify
uncertainty in the correlated variables.

Multiple regression using SuSiE
-------------------------------

Next, we fit a SuSiE model. (For simplicity, we fit a SuSiE model with
$L = 2$, but SuSiE will obtain similar results for $L > 2$.)

```{r fit-susie}
fit2 <- susie(X,y,L = 2,standardize = FALSE,estimate_prior_variance = FALSE,
              scaled_prior_variance = 1,min_abs_corr = 0,tol = 1e-8)
print(susie_get_cs(fit2,X,min_abs_corr = 0)$cs)
```

SuSiE identified two Credible Sets, in which $x_5$ is included in
both, so we have:

$$(\beta_1 \neq 0 \mbox{ or } \beta_2 \neq 0 \mbox{ or } \beta_5 \neq
0) \mbox{ and } (\beta_3 \neq 0 \mbox{ or } \beta_4 \neq 0 \mbox{ or }
\beta_5 \neq 0) )$$

Looking more closely at the posterior inclusion probabilities, most of
the weight is assigned to $x_3$ and $x_4$ in the first Credible Set,
and to $x_1$ and $x_2$ in the second Credible Set, but, in light of
the strong correlations with $x_5$, *each Credible Set also allows for
the possibility that $x_5$ affects the outcome.*

```{r susie-inclusion-probabilities}
round(fit2$alpha,digits = 3)
```

Therefore, although this statement is not the same as, "among
coefficients $\beta_1, \ldots, \beta_5$, two (or more) are not
zero"---for example, it allows for only selecting one variable,
$x_5$---it is does achieve the aim of capturing all relevant
variables.
