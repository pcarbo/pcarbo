---
title: "Linear vs. nonlinear embeddings: a thought experiment"
author: Peter Carbonetto
output: workflowr::wflow_html
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

Load the packages used in the analysis below, and set the seed.

```{r setup, message=FALSE}
library(mvtnorm)
library(Rtsne)
library(flashier)
library(ggplot2)
library(cowplot)
set.seed(2)
```

Consider the following data set. The 2-d data vectors are simulated so
that they cluster into two sets. The second cluster varies much more
along the first and second dimensions.

```{r sim-data, fig.width=3.5, fig.height=2.5}
X <- rbind(rmvnorm(300,c(-2,+2),diag(2)/30),
           rmvnorm(1000,c(0,0),rbind(c(3,2.9),
                                     c(2.9,3))))
cluster <- factor(c(rep(1,300),rep(2,1000)))
colnames(X) <- c("x1","x2")
p1 <- qplot(X[,1],X[,2],shape = I(21),color = I("white"),fill = cluster) +
  scale_fill_manual(values = c("tomato","dodgerblue")) +
  xlim(-7,7) +
  ylim(-7,7) +
  labs(x = "x1",y = "x2") +
  theme_cowplot(font_size = 12)
print(p1)
```

*Question:* Is there a (linear) matrix factorization method that
will produce a 1-d representation that correctly subdivides the points
into the two clusters?

Unsurprisingly, PCA doesn't work---the first PC picks up variation in
the second cluster.

```{r pca, fig.width=3, fig.height=2}
pca <- drop(prcomp(X,scale. = TRUE)$x[,1])
pdat <- data.frame(cluster,pca)
p2 <- ggplot(pdat,aes(x = cluster,y = pca)) +
  geom_boxplot(width = 0.25) +
  theme_cowplot(font_size = 12)
print(p2)
```

On the other hand, the 1-d embedding generated by *t*-SNE is able to
clearly distinguish the two clusters.

```{r tsne, fig.width=3, fig.height=2.5}
tsne <- drop(Rtsne(X,dims = 1,perplexity = 30,theta = 0.1,max_iter = 1000,
                   eta = 200,normalize = FALSE,pca = FALSE,verbose = FALSE)$Y)
pdat <- data.frame(cluster,tsne)
p3 <- ggplot(pdat,aes(x = cluster,y = tsne)) +
  geom_boxplot(width = 0.25) +
  theme_cowplot(font_size = 12)
print(p3)
```

FLASH with the default prior choices does no better than PCA in
separating the two clusters.

```{r flash-1, fig.width=3, fig.height=2.5}
fl1  <- flash(X,greedy.Kmax = 1,nullcheck = FALSE,verbose.lvl = 0)
y    <- drop(fl1$loadings.pm[[1]])
pdat <- data.frame(cluster,flash = y)
p4 <- ggplot(pdat,aes(x = cluster,y = flash)) +
  geom_boxplot(width = 0.25) +
  theme_cowplot(font_size = 12)
print(p4)
```

By contrast, FLASH with a unimodal prior on the loadings *can*
separate the two clusters so long as a random initialization is used.

```{r flash-2, fig.width=3, fig.height=2.5}
fl2 <- flash.init(X)
fl2 <- flash.init.factors(fl2,list(matrix(rnorm(nrow(X))),
                                   matrix(rnorm(ncol(X)))),
                          prior.family = c(prior.unimodal(),prior.normal()))
fl2 <- flash.backfit(fl2)
y   <- drop(fl2$loadings.pm[[1]])
pdat <- data.frame(cluster,flash = y)
p5 <- ggplot(pdat,aes(x = cluster,y = flash)) +
  geom_boxplot(width = 0.25) +
  theme_cowplot(font_size = 12)
print(p5)
```
